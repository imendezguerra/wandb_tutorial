{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imendezguerra/wandb_tutorial/blob/main/wandb_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGUZqnKKnxsL"
      },
      "source": [
        "# Weights and biases (W&B) tutorial\n",
        "Resources:\n",
        "- [W&B main page](https://wandb.ai)\n",
        "- [W&B documentation](https://docs.wandb.ai)\n",
        "- [W&B courses](https://https://wandb.ai/site/courses/)\n",
        "\n",
        "Weights and biases (W&B) is a popular MLOps platform used to track and manage machine-learning experiments. It aims to bring order, reproducibility, and efficiency to the chaotic world of ML experimentation.\n",
        "\n",
        "To do this W&B provides (among other specialised features):\n",
        "- Experiment tracking\n",
        "- Model and dataset versioning\n",
        "- Metric and data interactive visualisations\n",
        "- Hyperparameter sweeps\n",
        "- Collaborative environment\n",
        "- Integrations with common AI-tools (PyTorch, TensorFlow, JAX, Keras, HuggingFace, etc.)\n",
        "\n",
        "Together, these capabilities make ML research faster, more reliable, reproducible, scalable, and collaborative.\n",
        "\n",
        "In this tutorial we will cover the basic functions going through:\n",
        "- W&B registration and installation\n",
        "- logging, downloading, and using artifacts\n",
        "- logging models, metadata, and metrics\n",
        "- performing hyperparameter sweeps\n",
        "- visualising metrics (W&B interactive dashboard)\n",
        "- creating a report (W&B)\n",
        "\n",
        "The notebook is structured as follows:\n",
        "1. [W&B registration](#1-wb-registration)\n",
        "2. [W&B installation](#2-wb-installation)\n",
        "3. [Dataset preparation](#3-prepare-dataset)\n",
        "4. [Model training](#4-model-training)\n",
        "5. [Model optimisation](#5-model-optimisation)\n",
        "6. [Model evaluation](#6-model-evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsxAuD_rS3MX"
      },
      "source": [
        "## 1. W&B registration\n",
        "W&B offers a free forever plan for academic research including:\n",
        "- coordinating projects remotely\n",
        "- unlimited tracking hours, teams, projects\n",
        "- 100GB free cloud storage\n",
        "\n",
        "Sign-up in [wandb.ai](https://wandb.ai) using your academic email to unlock the benefits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b7KQiG-AS3MX"
      },
      "source": [
        "## 2. W&B installation\n",
        "First, replicate the environment using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nbdDne3NS3MY"
      },
      "outputs": [],
      "source": [
        "# conda env create -f environment.yml # Recreate the environment to run this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C44o3YAS3MZ"
      },
      "source": [
        "To install the `wandb` package run the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SuKTJKvcS3Ma"
      },
      "outputs": [],
      "source": [
        "# pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AnjfyLMS3Ma"
      },
      "source": [
        "Prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gNSuvN3eS3Ma",
        "outputId": "59bc4c8d-a30d-40bf-d198-cbf64b273055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'loguru'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3977865515.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloguru\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'loguru'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from loguru import logger\n",
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from src.dataset import generate_dataset, get_dataloaders_from_artifact\n",
        "from src.model import MLP, get_device\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "WANDB_TEAM = 'icl_img'\n",
        "WANDB_USER = 'im4417'\n",
        "PROJECT = 'wandb_tutorial'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsfxwEEoS3Mb"
      },
      "source": [
        "To authenticate your machine with W&B, generate an API key from your user profile at wandb.ai/authorize. Copy the API key and store it securely. Then run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWt7ZOebS3Mb"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFHb6QfBS3Mc"
      },
      "source": [
        "## 3. Prepare dataset\n",
        "Here we will :\n",
        "1. generate a synthetic dataset for further use\n",
        "2. split it into train/val/test,\n",
        "3. save it to disk as `.npz`,\n",
        "4. log it as a W&B Artifact of type `dataset`,\n",
        "\n",
        "To do this, the notebook uses functions from `src\\wandb_tutorial`. In this case the dataset comprises:\n",
        "- 2D datapoints  \n",
        "- with moderate overlap  \n",
        "- including 3 clusters\n",
        "\n",
        "The important part of this section is to showcase how to log artifacts in wandb for data versioning. To do so, take a look at the function below. Note that multiple datatypes (files) can be stored under the same artifact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3hS3xxvS3Mc"
      },
      "outputs": [],
      "source": [
        "def log_artifact_wandb(\n",
        "        files: list[str],\n",
        "        files_path: Path,\n",
        "        description: str,\n",
        "        artifact_type: str,\n",
        "        run: Optional[wandb.Run]=None\n",
        "    ):\n",
        "    \"\"\"Log dataset into wandb.\"\"\"\n",
        "    # Initialise run to upload data if run not provided\n",
        "    if not run:\n",
        "        run = wandb.init(project=PROJECT)\n",
        "    # Define artifact (equivalent to a folder)\n",
        "    artifact = wandb.Artifact(\n",
        "        name=files_path.name,\n",
        "        type=artifact_type,\n",
        "        description=description,\n",
        "    )\n",
        "    # Add files to remote artifact\n",
        "    for file in files:\n",
        "        artifact.add_file(files_path / file)\n",
        "\n",
        "    # Log artifact (equivalent to commit)\n",
        "    run.log_artifact(artifact)\n",
        "    # Finish the run\n",
        "    run.finish()\n",
        "    logger.success(\"Artifact successfully uploaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I-Bb0sBS3Md"
      },
      "outputs": [],
      "source": [
        "# Define data config and output path\n",
        "output_path = Path('data', 'blob_dataset')\n",
        "config = {\n",
        "    'seed': 42,\n",
        "    'n_samples': 1000,\n",
        "    'n_features': 2,\n",
        "    'n_classes': 4,\n",
        "    'cluster_std': 3,\n",
        "    'val_size': 0.1,\n",
        "    'test_size': 0.1,\n",
        "}\n",
        "\n",
        "# Generate synthetic dataset and store it locally\n",
        "generate_dataset(config, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntGMpQrdS3Md"
      },
      "outputs": [],
      "source": [
        "# Log artifacts into wandb\n",
        "log_artifact_wandb(\n",
        "    files = [\n",
        "        'data_train.npz', 'data_val.npz', 'data_test.npz',\n",
        "        'data_split.png', 'data_class_split.png', 'config.yml'\n",
        "    ],\n",
        "    files_path = output_path,\n",
        "    artifact_type = 'dataset',\n",
        "    description = 'Blob dataset split into train/val/test.'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO_xgp8_S3Me"
      },
      "source": [
        "## 4. Model training\n",
        "In this section we will:\n",
        "1. Use the dataset generated before to\n",
        "2. Train an MLP to predict the corresponding classes\n",
        "3. Log model hyperparameters (config) along with performance metrics during training\n",
        "4. Save final model checkpoint as another artifact\n",
        "\n",
        "Take a look at the function below to see how to download artifacts stored in wandb:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdfsthZ6S3Me"
      },
      "outputs": [],
      "source": [
        "\n",
        "def download_artifact_wandb(\n",
        "    path_to_artifact: str,\n",
        "    artifact_type: str,\n",
        "    artifact_version: Optional[str]=None,\n",
        "    run: Optional[wandb.Run] = None\n",
        ") -> wandb.Artifact:\n",
        "    \"\"\"Download artifact from wandb.\"\"\"\n",
        "    # Initialise run if not provided\n",
        "    if not run:\n",
        "        run = wandb.init()\n",
        "        init_run = True\n",
        "    else:\n",
        "        init_run = False\n",
        "    # Specify the version of the dataset, defaults to latest\n",
        "    artifact_version = 'latest' if artifact_version is None else artifact_version\n",
        "    # Make full path\n",
        "    artifact_dir = f'{WANDB_TEAM}/{PROJECT}/{path_to_artifact}:{artifact_version}'\n",
        "    logger.info(artifact_dir)\n",
        "    # Ensure artifact exists\n",
        "    if artifact_exists(artifact_dir):\n",
        "        # Point to artifact and retrieve path (download)\n",
        "        artifact = run.use_artifact(artifact_dir, type=artifact_type)\n",
        "        artifact_dir = artifact.download()\n",
        "        logger.success(f'Artifact {path_to_artifact} successfully downloaded.')\n",
        "    else:\n",
        "        logger.error(f'Artifact {path_to_artifact} does not exist.')\n",
        "    # Finish run if initialised in function\n",
        "    if init_run:\n",
        "        run.finish()\n",
        "    return artifact_dir\n",
        "\n",
        "def artifact_exists(artifact_ref: str) -> bool:\n",
        "    \"\"\"Check if an artifact exists in wandb.\"\"\"\n",
        "    api = wandb.Api()\n",
        "    try:\n",
        "        _ = api.artifact(artifact_ref)\n",
        "        return True\n",
        "    except wandb.errors.CommError:\n",
        "        # No run active\n",
        "        return False\n",
        "    except wandb.errors.ArtifactNotFoundError:\n",
        "        # No matching artifact\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PjJNT7jS3Mf"
      },
      "source": [
        "The following code shows the general syntax to log metrics in wandb during model training/validation/testing. For real-time visulalisation, implement it in the epoch loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec63K5wJS3Mf"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def mode_check(mode):\n",
        "    if mode == \"train\":\n",
        "        with torch.enable_grad():\n",
        "            yield\n",
        "    else:  # \"eval\", \"test\", \"validation\"\n",
        "        with torch.no_grad():\n",
        "            yield\n",
        "\n",
        "\n",
        "def run_one_epoch(model, loader, optimizer, mode):\n",
        "    # Set model mode\n",
        "    if mode:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    # Initialise variables\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with mode_check(mode):\n",
        "        for step, (X_batch, y_batch) in enumerate(loader):\n",
        "\n",
        "            # Run model for every batch\n",
        "            logits = model(X_batch)\n",
        "            loss = F.cross_entropy(logits, y_batch)\n",
        "\n",
        "            # Compute gradients and update model\n",
        "            if mode == 'train':\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Compute loss\n",
        "            total_loss += loss.item() * X_batch.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += X_batch.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "depbAe6zS3Mf"
      },
      "outputs": [],
      "source": [
        "def run_model(config=None):\n",
        "    if config is None:\n",
        "        config = {\n",
        "            \"epochs\": 20,\n",
        "            \"batch_size\": 64,\n",
        "            \"lr\": 1e-3,\n",
        "            \"input_dim\": 2,\n",
        "            \"hidden_dim\": 64,\n",
        "            \"output_dim\": 4,\n",
        "            \"dropout\": 0.1,\n",
        "            \"dataset_artifact\": \"blob_dataset\",\n",
        "            \"device\": \"cuda\",\n",
        "            \"dtype\": \"float32\",\n",
        "        }\n",
        "\n",
        "    # Check dtype and device\n",
        "    config[\"device\"] = get_device()\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=PROJECT,    # Initialise run in the project\n",
        "        config=config,      # Automatically save config\n",
        "        job_type=\"train\"    # Tag for job type\n",
        "    )\n",
        "\n",
        "    # Retrieve artifact remote path\n",
        "    artifact_dir = download_artifact_wandb(\n",
        "        path_to_artifact = config['dataset_artifact'],\n",
        "        artifact_type = 'dataset',\n",
        "        run = run\n",
        "    )\n",
        "    # Load and format dataset artifact into DataLoader\n",
        "    train_loader, val_loader, test_loader = get_dataloaders_from_artifact(\n",
        "        artifact_dir=artifact_dir, batch_size=config['batch_size'],\n",
        "        device=config[\"device\"]\n",
        "    )\n",
        "\n",
        "    model = MLP(\n",
        "        input_dim=config['input_dim'],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        output_dim=config['output_dim'],\n",
        "        dropout=config['dropout']\n",
        "    ).to(device=config[\"device\"])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    model_name = \"best_base_model.pt\"\n",
        "    model_path = Path(\"checkpoints\")\n",
        "    model_path.mkdir(exist_ok=True)\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        # Train and validate model per epoch\n",
        "        train_loss, train_acc = run_one_epoch(\n",
        "            model, train_loader, optimizer, mode=\"train\"\n",
        "        )\n",
        "        val_loss, val_acc = run_one_epoch(\n",
        "            model, val_loader, optimizer, mode=\"val\"\n",
        "        )\n",
        "\n",
        "        # Log results locally\n",
        "        logger.info(\n",
        "            f\"Epoch {epoch}: \"\n",
        "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, \"\n",
        "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n",
        "        )\n",
        "\n",
        "        # W&B log\n",
        "        for prefix, loss, acc in (\n",
        "            (\"train\", train_loss, train_acc),\n",
        "            (\"val\", val_loss, val_acc),\n",
        "        ):\n",
        "            if run is not None:\n",
        "                run.log({\n",
        "                    f\"{prefix}/loss\": loss,\n",
        "                    f\"{prefix}/acc\": acc,\n",
        "                })\n",
        "\n",
        "        # Track best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), model_path/model_name)\n",
        "            run.log({\"best_val_acc\": best_val_acc})\n",
        "\n",
        "    # Log the best model as an artifact\n",
        "    log_artifact_wandb(\n",
        "        files = [model_name],\n",
        "        files_path = model_path,\n",
        "        artifact_type = 'model',\n",
        "        description = 'Best baseline model according to validation accuracy',\n",
        "        run=run\n",
        "    )\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "\n",
        "# Run one baseline experiment\n",
        "model_config = {\n",
        "    \"epochs\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": 1e-3,\n",
        "    \"input_dim\": 2,\n",
        "    \"hidden_dim\": 64,\n",
        "    \"output_dim\": 4,\n",
        "    \"dropout\": 0.1,\n",
        "    \"dataset_artifact\": \"blob_dataset\",\n",
        "    \"device\": \"cuda\",\n",
        "    \"dtype\": \"float32\",\n",
        "}\n",
        "run_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNKEqf8RS3Mg"
      },
      "source": [
        "## 5. Model optimisation\n",
        "\n",
        "Now we will use **W&B Sweeps** to explore hyperparameters:\n",
        "- learning rate (`lr`)\n",
        "- hidden layer size (`hidden_dim`)\n",
        "- dropout\n",
        "\n",
        "Workflow:\n",
        "\n",
        "1. Define a **sweep configuration** (search space + objective).\n",
        "2. Define a training function `sweep_train()` that:\n",
        "   - reads hyperparameters from `wandb.config`\n",
        "   - trains & logs metrics\n",
        "\n",
        "In the W&B UI, you’ll see:\n",
        "- individual runs\n",
        "- parallel coordinate plots\n",
        "- best hyperparameter configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7fytWOsS3Mh"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"bayes\",      # for bayesian optimisation or \"grid\" for grid-search or \"random\" for random-search\n",
        "    \"early_terminate\": {    # early stopping configuration\n",
        "        \"type\": \"hyperband\",\n",
        "        \"min_iter\": 5,\n",
        "        \"max_iter\": 50,\n",
        "        \"s\": 2,\n",
        "    },\n",
        "    \"metric\": {             # Metric to optimse\n",
        "        \"name\": \"val/loss\",\n",
        "        \"goal\": \"minimize\"\n",
        "    },\n",
        "    \"parameters\": {         # Parameters to be modified\n",
        "\n",
        "        \"epochs\": {\n",
        "            \"value\": 20     # Fixed parameter that would be stored with sweep\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \"lr\": {\n",
        "            \"values\": [1e-2, 3e-3, 1e-3, 3e-4]\n",
        "        },\n",
        "        \"hidden_dim\": {\n",
        "            \"values\": [32, 64, 128] # Fixed number of possible values\n",
        "        },\n",
        "        \"dropout\": { # Continuous sampling within range\n",
        "            \"min\": 0.0,\n",
        "            \"max\": 0.3\n",
        "        },\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFaFW9VVS3Mh"
      },
      "outputs": [],
      "source": [
        "# Register the config sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
        "def sweep_run():\n",
        "    \"\"\"Link execution function with generated sampling config\"\"\"\n",
        "    wandb.init(project=PROJECT)\n",
        "    run_model(model_config.update(dict(wandb.config)))\n",
        "# Run the registered sweep with the corresponding execution function\n",
        "wandb.agent(sweep_id, function=sweep_run, count=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUJTh1oJS3Mi"
      },
      "source": [
        "## 6. Model evaluation\n",
        "In wandb [interactive dashboard](https://wandb.ai/icl_img/wandb_tutorial/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}