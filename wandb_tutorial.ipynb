{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/imendezguerra/wandb_tutorial/blob/main/wandb_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGUZqnKKnxsL"
   },
   "source": [
    "# Weights and biases (W&B) tutorial\n",
    "Resources:\n",
    "- [W&B main page](https://wandb.ai)\n",
    "- [W&B documentation](https://docs.wandb.ai)\n",
    "- [W&B courses](https://https://wandb.ai/site/courses/)\n",
    "\n",
    "Weights and biases (W&B) is a popular MLOps platform used to track and manage machine-learning experiments. It aims to bring order, reproducibility, and efficiency to the chaotic world of ML experimentation.\n",
    "\n",
    "To do this W&B provides (among other specialised features):\n",
    "- Experiment tracking\n",
    "- Model and dataset versioning\n",
    "- Metric and data interactive visualisations\n",
    "- Hyperparameter sweeps\n",
    "- Collaborative environment\n",
    "- Integrations with common AI-tools (PyTorch, TensorFlow, JAX, Keras, HuggingFace, etc.)\n",
    "\n",
    "Together, these capabilities make ML research faster, more reliable, reproducible, scalable, and collaborative.\n",
    "\n",
    "In this tutorial we will cover the basic functions going through:\n",
    "- W&B registration and installation\n",
    "- logging, downloading, and using artifacts\n",
    "- logging models, metadata, and metrics\n",
    "- performing hyperparameter sweeps\n",
    "- visualising metrics (W&B interactive dashboard)\n",
    "- creating a report (W&B)\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1. [W&B registration](#1-wb-registration)\n",
    "2. [W&B installation](#2-wb-installation)\n",
    "3. [Dataset preparation](#3-prepare-dataset)\n",
    "4. [Model training](#4-model-training)\n",
    "5. [Model optimisation](#5-model-optimisation)\n",
    "6. [Model evaluation](#6-model-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. W&B registration\n",
    "W&B offers a free forever plan for academic research including:\n",
    "- coordinating projects remotely\n",
    "- unlimited tracking hours, teams, projects\n",
    "- 100GB free cloud storage\n",
    "\n",
    "Sign-up in [wandb.ai](https://wandb.ai) using your academic email to unlock the benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. W&B installation\n",
    "First, replicate the environment using:\n",
    "\n",
    "`!git clone https://github.com/imendezguerra/wandb_tutorial.git`\n",
    "\n",
    "`%cd wandb_tutorial`\n",
    "\n",
    "`!pip install -r requirements.txt`\n",
    "\n",
    "`import sys`\n",
    "\n",
    "`sys.path.append('src')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the `wandb` package specifically run: `pip install wandb`\n",
    "In this case the package is already included in `requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from typing import Optional \n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from src.dataset import generate_dataset, get_dataloaders_from_artifact\n",
    "from src.model import MLP, get_device, run_one_epoch, set_random_seed\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "WANDB_TEAM = 'icl_img'\n",
    "WANDB_USER = 'im4417'\n",
    "PROJECT = 'wandb_tutorial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To authenticate your machine with W&B, generate an API key from your user profile at wandb.ai/authorize. Copy the API key and store it securely. Then run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare dataset \n",
    "Here we will :\n",
    "1. generate a synthetic dataset for further use\n",
    "2. split it into train/val/test,\n",
    "3. save it to disk as `.npz`,\n",
    "4. log it as a W&B Artifact of type `dataset`,\n",
    "\n",
    "To do this, the notebook uses functions from `src\\wandb_tutorial`. In this case the dataset comprises:\n",
    "- 2D datapoints  \n",
    "- with moderate overlap  \n",
    "- including 3 clusters \n",
    "\n",
    "The important part of this section is to showcase how to log artifacts in wandb for data versioning. To do so, take a look at the function below. Note that multiple datatypes (files) can be stored under the same artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_artifact_wandb(\n",
    "        files: list[str],\n",
    "        files_path: Path,\n",
    "        description: str,\n",
    "        artifact_type: str,\n",
    "        run: Optional[wandb.Run]=None\n",
    "    ):\n",
    "    \"\"\"Log dataset into wandb.\"\"\"\n",
    "    # Initialise run to upload data if run not provided\n",
    "    if not run:\n",
    "        run = wandb.init(project=PROJECT)\n",
    "    # Define artifact (equivalent to a folder)\n",
    "    artifact = wandb.Artifact(\n",
    "        name=files_path.name,\n",
    "        type=artifact_type,\n",
    "        description=description,\n",
    "    )\n",
    "    # Add files to remote artifact\n",
    "    for file in files:\n",
    "        artifact.add_file(files_path / file) \n",
    "\n",
    "    # Log artifact (equivalent to commit)\n",
    "    run.log_artifact(artifact)\n",
    "    # Finish the run\n",
    "    run.finish()\n",
    "    logger.success(f\"Artifact {artifact_type} type successfully uploaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data config and output path\n",
    "output_path = Path('data', 'blob_dataset')\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'n_samples': 1000,\n",
    "    'n_features': 2,\n",
    "    'n_classes': 4,\n",
    "    'cluster_std': 3,\n",
    "    'val_size': 0.1,\n",
    "    'test_size': 0.1,\n",
    "}\n",
    "\n",
    "# Generate synthetic dataset and store it locally\n",
    "generate_dataset(config, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log artifacts into wandb\n",
    "log_artifact_wandb(\n",
    "    files = [\n",
    "        'data_train.npz', 'data_val.npz', 'data_test.npz',\n",
    "        'data_split.png', 'data_class_split.png', 'config.yml'\n",
    "    ],\n",
    "    files_path = output_path,\n",
    "    artifact_type = 'dataset',\n",
    "    description = 'Blob dataset split into train/val/test.'  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training\n",
    "In this section we will:\n",
    "1. Use the dataset generated before to\n",
    "2. Train an MLP to predict the corresponding classes\n",
    "3. Log model hyperparameters (config) along with performance metrics during training \n",
    "4. Save final model checkpoint as another artifact\n",
    "\n",
    "Take a look at the function below to see how to download artifacts stored in wandb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_artifact_wandb(\n",
    "    path_to_artifact: str,\n",
    "    artifact_type: str,\n",
    "    artifact_version: Optional[str]=None,\n",
    "    run: Optional[wandb.Run] = None\n",
    ") -> wandb.Artifact:\n",
    "    \"\"\"Download artifact from wandb.\"\"\"\n",
    "    # Initialise run if not provided\n",
    "    if not run:\n",
    "        run = wandb.init()\n",
    "        init_run = True\n",
    "    else:\n",
    "        init_run = False\n",
    "    # Specify the version of the dataset, defaults to latest\n",
    "    artifact_version = 'latest' if artifact_version is None else artifact_version\n",
    "    # Make full path to the aritifact\n",
    "    artifact_dir = f'{WANDB_TEAM}/{PROJECT}/{path_to_artifact}:{artifact_version}'\n",
    "    # Ensure artifact exists\n",
    "    if artifact_exists(artifact_dir):\n",
    "        # Point to artifact and retrieve path (download)\n",
    "        artifact = run.use_artifact(artifact_dir, type=artifact_type)\n",
    "        artifact_dir = artifact.download()\n",
    "        logger.success(f'Artifact {path_to_artifact}:{artifact_version} successfully downloaded.')\n",
    "    else:\n",
    "        logger.error(f'Artifact {path_to_artifact}:{artifact_version} does not exist.')\n",
    "    # Finish run if initialised in function\n",
    "    if init_run:\n",
    "        run.finish()\n",
    "    return artifact_dir\n",
    "\n",
    "def artifact_exists(artifact_ref: str) -> bool:\n",
    "    \"\"\"Check if an artifact exists in wandb.\"\"\"\n",
    "    api = wandb.Api()\n",
    "    try:\n",
    "        _ = api.artifact(artifact_ref)\n",
    "        return True\n",
    "    except wandb.errors.CommError:\n",
    "        # No run active\n",
    "        return False\n",
    "    except wandb.errors.ArtifactNotFoundError:\n",
    "        # No matching artifact\n",
    "        return False  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows a full run of a model training and validation. Note the framed sections showing how to:\n",
    "- connect to wandb\n",
    "- download dataset artifact generated before\n",
    "- log train/val metrics on the fly \n",
    "- log best performing model as an artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(config=None):\n",
    "    if config is None:\n",
    "        config = {\n",
    "            \"seed\": 42,\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 64,\n",
    "            \"lr\": 1e-3,\n",
    "            \"input_dim\": 2,\n",
    "            \"hidden_dim\": 64,\n",
    "            \"output_dim\": 4,\n",
    "            \"dropout\": 0.1,\n",
    "            \"dataset_artifact\": \"blob_dataset\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"dtype\": \"float32\",\n",
    "        }\n",
    "\n",
    "    # Check dtype and device\n",
    "    config[\"device\"] = get_device()\n",
    "    logger.debug(config)\n",
    "    \n",
    "    # Set seed\n",
    "    set_random_seed(config[\"seed\"])\n",
    "\n",
    "    # --------------- W&B initialise run (connect) ----------------\n",
    "    run = wandb.init(\n",
    "        project=PROJECT,    # Initialise run in the project\n",
    "        config=config,      # Automatically save config\n",
    "    )\n",
    "    # --------------------------------------------------------------\n",
    "    # ----- W&B download artifact (retrieves remote path only) -----\n",
    "    artifact_dir = download_artifact_wandb(\n",
    "        path_to_artifact = config['dataset_artifact'],\n",
    "        artifact_type = 'dataset',\n",
    "        run = run\n",
    "    )\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # Load and format dataset artifact into DataLoader\n",
    "    train_loader, val_loader, test_loader = get_dataloaders_from_artifact(\n",
    "        artifact_dir=artifact_dir, batch_size=config['batch_size'],\n",
    "        device=config[\"device\"]\n",
    "    )\n",
    "\n",
    "    model = MLP(\n",
    "        input_dim=config['input_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        output_dim=config['output_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device=config[\"device\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    model_name = \"MLP_state_dict.pt\"\n",
    "    model_path = Path(\"checkpoints\")\n",
    "    model_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        # Run model through training set across batches (train mode)\n",
    "        train_loss, train_acc, batch_metrics = run_one_epoch(\n",
    "            model, train_loader, optimizer, epoch, mode=\"train\"\n",
    "        )\n",
    "        # Run model through validation set across batches (eval mode)\n",
    "        val_loss, val_acc, _ = run_one_epoch(\n",
    "            model, val_loader, optimizer, epoch, mode=\"eval\"\n",
    "        )\n",
    "\n",
    "        # ------- W&B log metrics about training and validation -------\n",
    "        # Batch metrics come as vectors so need to unwrap them for logging.\n",
    "        # This step is included here for clarity but would be better implemented\n",
    "        # in run_one_epoch directly\n",
    "        if run is not None:\n",
    "            for v in batch_metrics['loss_batch']:\n",
    "                run.log({\"train/loss_batch\": v}) \n",
    "        # Overall epoch results for training and validation\n",
    "        for prefix, loss, acc in (\n",
    "            (\"train\", train_loss, train_acc),\n",
    "            (\"val\", val_loss, val_acc),\n",
    "        ):\n",
    "            if run is not None:\n",
    "                run.log({\n",
    "                    f\"{prefix}/loss_epoch\": loss,\n",
    "                    f\"{prefix}/acc_epoch\": acc,\n",
    "                })\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_path/model_name)\n",
    "            run.log({\"best_val_acc\": best_val_acc})\n",
    "\n",
    "    # ----------------- W&B log model as artifact -----------------\n",
    "    log_artifact_wandb(\n",
    "        files = [model_name],\n",
    "        files_path = model_path,\n",
    "        artifact_type = 'model',\n",
    "        description = 'Best baseline model according to validation accuracy',\n",
    "        run=run\n",
    "    )\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run one baseline experiment\n",
    "model_config = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"input_dim\": 2,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"output_dim\": 4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"dataset_artifact\": \"blob_dataset\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"dtype\": \"float32\",\n",
    "}\n",
    "run_model(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model optimisation\n",
    "\n",
    "Now we will use **W&B Sweeps** to explore hyperparameters:\n",
    "- learning rate (`lr`)\n",
    "- hidden layer size (`hidden_dim`)\n",
    "- dropout\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. Define a **sweep configuration** (search space + objective).\n",
    "2. Define a training function `sweep_train()` that:\n",
    "   - reads hyperparameters from `wandb.config`\n",
    "   - trains & logs metrics\n",
    "\n",
    "In the W&B UI, you’ll see:\n",
    "- individual runs\n",
    "- parallel coordinate plots\n",
    "- best hyperparameter configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",      # for bayesian optimisation or \"grid\" for grid-search or \"random\" for random-search\n",
    "    \"early_terminate\": {    # early stopping configuration\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 5,\n",
    "        \"max_iter\": 50,\n",
    "        \"s\": 2,\n",
    "    },\n",
    "    \"metric\": {             # Metric to optimse\n",
    "        \"name\": \"val/loss_epoch\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {         # Parameters to be modified\n",
    "\n",
    "        \"epochs\": {\n",
    "            \"value\": 20     # Fixed parameter that would be stored with sweep\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32, 64, 128]\n",
    "        },\n",
    "        \"lr\": {\n",
    "            \"values\": [1e-2, 3e-3, 1e-3, 3e-4]\n",
    "        },\n",
    "        \"hidden_dim\": { \n",
    "            \"values\": [32, 64, 128] # Fixed number of possible values\n",
    "        },\n",
    "        \"dropout\": { # Continuous sampling within range\n",
    "            \"min\": 0.0,\n",
    "            \"max\": 0.3\n",
    "        },\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the config sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT)\n",
    "def sweep_run():\n",
    "    \"\"\"Link execution function with generated sampling config\"\"\"\n",
    "    wandb.init(project=PROJECT)\n",
    "    # Merge base model config with sweep config\n",
    "    curr_sweep_config = {**model_config, **wandb.config.as_dict()}\n",
    "    # Pass config to the run\n",
    "    run_model(curr_sweep_config)\n",
    "# Run the registered sweep with the corresponding execution function\n",
    "wandb.agent(sweep_id, function=sweep_run, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model evaluation\n",
    "Now we can inspec the models' performance in [wandb interactive dashboard](https://wandb.ai/icl_img/wandb_tutorial/).\n",
    "\n",
    "We can also query this programatically to retrieve a specific model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to wandb\n",
    "api = wandb.Api()\n",
    "runs = api.runs(PROJECT)\n",
    "\n",
    "# Select runs that contain metric of interest\n",
    "valid_runs = [r for r in runs if \"val/loss_epoch\" in r.summary]\n",
    "if not valid_runs:\n",
    "    raise ValueError(\"No runs contain 'val/loss_epoch' in summary.\")\n",
    "\n",
    "# Apply metric criterion (in this case min(val/loss_epoch))\n",
    "best = min(valid_runs, key=lambda r: r.summary[\"val/loss_epoch\"])\n",
    "logger.info(f\"Best run: {best}\")\n",
    "\n",
    "# Retrieve remote path to artifact\n",
    "best_artifact = best.logged_artifacts()[0]\n",
    "artifact_dir = best_artifact.download()\n",
    "\n",
    "# Get config used to initialise and train the model artifact\n",
    "art_config = best_artifact.logged_by().config\n",
    "\n",
    "# Get file with the state_dict (in this case select by .pt extension)\n",
    "art_state_dict = [p for p in Path(artifact_dir).iterdir() if p.suffix == \".pt\"][0]\n",
    "logger.info(f\"Best model: {art_state_dict}\")\n",
    "logger.info(f\"Best model config: {art_config}\")\n",
    "\n",
    "# Initialise model using artifact config\n",
    "model = MLP(\n",
    "    input_dim = art_config['input_dim'],\n",
    "    hidden_dim = art_config['hidden_dim'],\n",
    "    output_dim = art_config['output_dim'],\n",
    "    dropout = art_config['dropout'],\n",
    ")\n",
    "state_dict = torch.load(art_state_dict)\n",
    "model.load_state_dict(state_dict)\n",
    "logger.success(f'Model ready to use:\\n{model}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPieSuo90pmFH7nIzxKN9J4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
